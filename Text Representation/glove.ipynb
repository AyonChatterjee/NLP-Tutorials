{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number od unique qords in dictionary = 6\n",
      "Dictionary is =  {'text': 1, 'language': 2, 'natural': 3, 'leader': 4, 'prime': 5, 'the': 6}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = {'text' , 'the' , 'leader' , 'prime' , 'natural' , 'language'}\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "print(\"Number od unique qords in dictionary =\" , len(tokenizer.word_index))\n",
    "\n",
    "print(\"Dictionary is = \" , tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding : 1 -> dense vector \n",
    "\n",
    "def embedding_for_vocab(filepath , word_index , embedding_dim):\n",
    "      vocab_size = len(word_index) + 1\n",
    "\n",
    "      embedding_matrix_vocab = np.zeros((vocab_size , embedding_dim))\n",
    "\n",
    "      with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "      return embedding_matrix_vocab     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector for first word is =>  [ 0.32615     0.36686    -0.0074905  -0.37553     0.66715002  0.21646\n",
      " -0.19801    -1.10010004 -0.42221001  0.10574    -0.31292     0.50953001\n",
      "  0.55774999  0.12019     0.31441    -0.25042999 -1.06369996 -1.32130003\n",
      "  0.87797999 -0.24627     0.27379    -0.51091999  0.49324     0.52243\n",
      "  1.16359997 -0.75322998 -0.48052999 -0.11259    -0.54595    -0.83920997\n",
      "  2.98250008 -1.19159997 -0.51958001 -0.39365    -0.1419     -0.026977\n",
      "  0.66295999  0.16574    -1.1681      0.14443     1.63049996 -0.17216\n",
      " -0.17436001 -0.01049    -0.17794     0.93076003  1.0381      0.94265997\n",
      " -0.14805    -0.61109   ]\n"
     ]
    }
   ],
   "source": [
    "# matrix for vocab : word_index\n",
    "\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab(\"glove.6B.50d.txt\" , tokenizer.word_index , embedding_dim)\n",
    "\n",
    "print(\"Dense vector for first word is => \" , embedding_matrix_vocab[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
